<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Jpalbino.github.io : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Jpalbino.github.io</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/jpalbino">View on GitHub</a>

          <h1 id="project_title">Jpalbino.github.io</h1>
          <h2 id="project_tagline"></h2>

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p>&lt;!DOCTYPE html&gt;

</p>
<p></p>

code{white-space: pre;}<p></p>


pre:not([class]) {
background-color: white;
}
<p>
</p>


.main-container {
max-width: 940px;
margin-left: auto;
margin-right: auto;
}
<div>
<div>
<h1>
<a name="practical-machine-learning-course-project" class="anchor" href="#practical-machine-learning-course-project"><span class="octicon octicon-link"></span></a>Practical Machine Learning Course Project</h1>
</div>

<div>
<h1>
<a name="executive-summary" class="anchor" href="#executive-summary"><span class="octicon octicon-link"></span></a>Executive Summary</h1>
<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self-movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. More information is available on the website: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>. In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbbell of six participants. They were asked to perform barbell lifts correctly and incorrectly in five different ways. The project’s goal is to predict the manner in which the participants did the exercise. In this report, we will describe how we built our model using cross validation, what are the expected out of sample error, and why we made the choices we did. In the prediction model, we predict 20 different test cases.</p>
</div>

<div>
<h1>
<a name="data" class="anchor" href="#data"><span class="octicon octicon-link"></span></a>Data</h1>
<p>The data for this project come from this source: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>. The training data are available here: <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a>, and the testing data are available here: <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a>.</p>
<pre><code># Setting the default workspace, in this case the directory is D:\A Practical Machine Learning\Peer Assessments\data.
setwd("D:/A Practical Machine Learning/Peer Assessments/data")
# To avoid error in the URLthat is of type https:// URLs we need to use --internet2.
setInternet2(TRUE)
# setInternet2(TRUE) are used if the certificate is considered to be valid.
# Downloading the two datasets:
downloaddata &lt;- function(url, nastrings) {
temp &lt;- tempfile()
download.file(url, temp)
data &lt;- read.csv(temp, na.strings = nastrings)
unlink(temp)
return(data)
}</code></pre>
</div>

<div>
<h1>
<a name="exploratory-data-analysis" class="anchor" href="#exploratory-data-analysis"><span class="octicon octicon-link"></span></a>Exploratory Data Analysis</h1>
<div>
<h2>
<a name="creating-training-test-and-validation-sets" class="anchor" href="#creating-training-test-and-validation-sets"><span class="octicon octicon-link"></span></a>Creating training, test and validation sets</h2>
<pre><code>trainurl &lt;- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
train &lt;- downloaddata(trainurl, c("", "NA", "#DIV/0!"))
testurl &lt;- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test &lt;- downloaddata(testurl, c("", "NA", "#DIV/0!"))
# verifying data
dim(train)</code></pre>
<pre><code>## [1] 19622 160</code></pre>
<pre><code>table(train$classe)</code></pre>
<pre><code>##
## A B C D E
## 5580 3797 3422 3216 3607</code></pre>
<p>As observed, training data has 19622 observations with 160 variables, and the distribution of the five measured stances (“classes”) named as A,B,C,D and E:</p>
<p>We have a large training set (19,622 entries) and a small testing set (20 entries). Instead of running the model throughout all training dataset once, as it would be time consuming, we decided to split the training dataset. We separate our training data into a training set and a validation set so that we can validate our model. We chose to divide the given training set into two sets, each of which was then split into a training set (comprising 80% of the entries) and a validation set (or test set) (comprising 20% of the entries).</p>
<pre><code>library(caret)</code></pre>
<pre><code>## Loading required package: lattice
## Loading required package: ggplot2</code></pre>
<pre><code>set.seed(123456)
trainset &lt;- createDataPartition(train$classe, p = 0.8, list = FALSE)
Training &lt;- train[trainset, ]
Validation &lt;- train[-trainset, ]</code></pre>
</div>

<div>
<h2>
<a name="feature-selection" class="anchor" href="#feature-selection"><span class="octicon octicon-link"></span></a>Feature selection</h2>
<p>First we clean up near zero variance features, columns with missing values and descriptive fields.</p>
<pre><code># exclude near zero variance features
nzvcol &lt;- nearZeroVar(Training)
Training &lt;- Training[, -nzvcol]
# exclude columns with 40% more missing values exclude descriptive columns like name etc
cntlength &lt;- sapply(Training, function(x) {
sum(!(is.na(x) | x == ""))
})
nullcol &lt;- names(cntlength[cntlength &lt; 0.6 * length(Training$classe)])
descriptcol &lt;- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",
"cvtd_timestamp", "new_window", "num_window")
excludecols &lt;- c(descriptcol, nullcol)
Training &lt;- Training[, !names(Training) %in% excludecols]</code></pre>
</div>

<div>
<h2>
<a name="model-training" class="anchor" href="#model-training"><span class="octicon octicon-link"></span></a>Model Training</h2>
<p>The selected model is the random forest, as implemented in the randomForest package (<a href="http://cran.r-project.org/web/packages/randomForest/index.html">http://cran.r-project.org/web/packages/randomForest/index.html</a>) based on Breiman and Cutler’s original Fortran code algorithm. Random forests are an ensemble learning method for classification (and regression) that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees.</p>
<pre><code>library(randomForest)</code></pre>
<pre><code>## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>rfModel &lt;- randomForest(classe ~ ., data = Training, importance = TRUE, ntrees = 10)</code></pre>
</div>

<div>
<h2>
<a name="model-validation" class="anchor" href="#model-validation"><span class="octicon octicon-link"></span></a>Model Validation</h2>
<p>In this point, we test our model performance on the training set itself and the cross validation set and show the results in Confusion Matrix layout. A Confusion Matrix, also known as a contingency table or an error matrix, is a specific table layout that allows visualization of the performance of an algorithm.</p>
<div>
<h3>
<a name="training-set-accuracy" class="anchor" href="#training-set-accuracy"><span class="octicon octicon-link"></span></a>Training set accuracy</h3>
<pre><code>ptraining &lt;- predict(rfModel, Training)
print(confusionMatrix(ptraining, Training$classe))</code></pre>
<pre><code>## Confusion Matrix and Statistics
##
## Reference
## Prediction A B C D E
## A 4464 0 0 0 0
## B 0 3038 0 0 0
## C 0 0 2738 0 0
## D 0 0 0 2573 0
## E 0 0 0 0 2886
##
## Overall Statistics
##
## Accuracy : 1
## 95% CI : (1, 1)
## No Information Rate : 0.284
## P-Value [Acc &gt; NIR] : &lt;2e-16
##
## Kappa : 1
## Mcnemar's Test P-Value : NA
##
## Statistics by Class:
##
## Class: A Class: B Class: C Class: D Class: E
## Sensitivity 1.000 1.000 1.000 1.000 1.000
## Specificity 1.000 1.000 1.000 1.000 1.000
## Pos Pred Value 1.000 1.000 1.000 1.000 1.000
## Neg Pred Value 1.000 1.000 1.000 1.000 1.000
## Prevalence 0.284 0.194 0.174 0.164 0.184
## Detection Rate 0.284 0.194 0.174 0.164 0.184
## Detection Prevalence 0.284 0.194 0.174 0.164 0.184
## Balanced Accuracy 1.000 1.000 1.000 1.000 1.000</code></pre>
<p>I seems that the model performs well in the training set, but we need cross validate the performance alongside the held out set and observe if it is not overfitting.</p>
</div>

<p></p>
</div>

<div>
<h2>
<a name="validation-set-accuracy-out-of-sample" class="anchor" href="#validation-set-accuracy-out-of-sample"><span class="octicon octicon-link"></span></a>Validation set accuracy (Out-of-Sample)</h2>
<p>Here we test the model performance on the cross validation set that was held out from training. Out-of-sample testing and forward performance testing provide further confirmation regarding a model’s effectiveness.</p>
<pre><code>pvalidation &lt;- predict(rfModel, Validation)
print(confusionMatrix(pvalidation, Validation$classe))</code></pre>
<pre><code>## Confusion Matrix and Statistics
##
## Reference
## Prediction A B C D E
## A 1116 7 0 0 0
## B 0 751 4 0 0
## C 0 1 680 4 0
## D 0 0 0 639 4
## E 0 0 0 0 717
##
## Overall Statistics
##
## Accuracy : 0.995
## 95% CI : (0.992, 0.997)
## No Information Rate : 0.284
## P-Value [Acc &gt; NIR] : &lt;2e-16
##
## Kappa : 0.994
## Mcnemar's Test P-Value : NA
##
## Statistics by Class:
##
## Class: A Class: B Class: C Class: D Class: E
## Sensitivity 1.000 0.989 0.994 0.994 0.994
## Specificity 0.998 0.999 0.998 0.999 1.000
## Pos Pred Value 0.994 0.995 0.993 0.994 1.000
## Neg Pred Value 1.000 0.997 0.999 0.999 0.999
## Prevalence 0.284 0.193 0.174 0.164 0.184
## Detection Rate 0.284 0.191 0.173 0.163 0.183
## Detection Prevalence 0.286 0.192 0.175 0.164 0.183
## Balanced Accuracy 0.999 0.994 0.996 0.996 0.997</code></pre>
<p>As we can observe, the cross validation accuracy is 99.5% and the out-of-sample error is therefore 0.5% so the model performance is quite good.</p>
</div>

<div>
<h2>
<a name="test-set-prediction" class="anchor" href="#test-set-prediction"><span class="octicon octicon-link"></span></a>Test set prediction</h2>
<p>The algorithm for prediction on the test set is:</p>
<pre><code>ptest &lt;- predict(rfModel, test)</code></pre>
</div>

<p></p>
</div>

<div>
<h1>
<a name="results" class="anchor" href="#results"><span class="octicon octicon-link"></span></a>Results</h1>
<pre><code>ptest</code></pre>
<pre><code>## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
## B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E</code></pre>
<p>Finally, saving the output to files according to instructions in the Prediction Assignment Writeup and post it to the submission page.</p>
<pre><code>answers &lt;- as.vector(ptest)
pml_write_files = function(x) {
n = length(x)
for (i in 1:n) {
filename = paste0("problem_id_", i, ".txt")
write.table(x[i], file = filename, quote = FALSE, row.names = FALSE,
col.names = FALSE)
}
}
pml_write_files(answers)</code></pre>
</div>

<p></p>
</div>

<p>
</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
